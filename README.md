# E-Commerce Price Monitoring System

Advanced multi-source data collection system for monitoring product prices across e-commerce platforms.

## ğŸ¥ Video Demonstration

**Watch the complete system demonstration:**

<div align="center">
  
https://github.com/user-attachments/assets/scraping_final_video.mp4

</div>

> **ğŸ“¹ Demo Video**: Complete walkthrough of all features including scraping, analysis, and report generation.
>
> **Alternative links**:
> - [Direct Download](./scraping_final_video.mp4)
> - [View in Browser](./scraping_final_video.mp4)

<details>
<summary>ğŸ“‹ <strong>Video Content Overview</strong></summary>

**What you'll see in the demo:**
- âœ… System setup and initialization
- âœ… Interactive scraping with `python main.py`
- âœ… Real-time data collection from Amazon, eBay, and Shop.ge
- âœ… Statistical analysis and price volatility detection
- âœ… HTML report generation with charts
- âœ… CLI commands demonstration
- âœ… Results viewing and data export

</details>

---

## Project Overview

This system scrapes product data from Amazon, eBay, and Walmart, performs statistical analysis, and generates reports. It uses concurrent processing, database storage, and a modular architecture.

## Features

- Scrapes at least 3 e-commerce sites (Amazon, eBay, Walmart)
- Supports both static (BeautifulSoup4) and dynamic (Selenium) scraping
- Uses Scrapy framework for at least one crawler
- Handles rate limiting and basic anti-bot measures
- Robust error handling and retry logic
- Concurrent scraping (multi-threaded)
- Database storage (SQLite via SQLAlchemy)
- Data cleaning, validation, and statistical analysis (pandas/numpy)
- Generates trend reports and exports data (CSV, JSON, Excel)
- Command-line interface for all operations
- Automated and interactive report generation

## Project Structure

```
final-project/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ scrapers/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base_scraper.py          # Abstract base scraper class
â”‚   â”‚   â”œâ”€â”€ static_scraper.py        # BeautifulSoup4 implementation
â”‚   â”‚   â”œâ”€â”€ selenium_scraper.py      # Selenium WebDriver implementation
â”‚   â”‚   â”œâ”€â”€ concurrent_manager.py    # Threading-based concurrent processing
â”‚   â”‚   â”œâ”€â”€ factory.py               # Scraper factory pattern
â”‚   â”‚   â”œâ”€â”€ data_models.py           # Data models and validation
â”‚   â”‚   â””â”€â”€ scrapy_crawler/          # Scrapy framework implementation
â”‚   â”‚       â”œâ”€â”€ items.py
â”‚   â”‚       â”œâ”€â”€ pipelines.py
â”‚   â”‚       â”œâ”€â”€ settings.py
â”‚   â”‚       â””â”€â”€ spiders/
â”‚   â”‚           â””â”€â”€ amazon_spider.py
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ models.py                # SQLAlchemy database models
â”‚   â”‚   â”œâ”€â”€ database.py              # Database connection and operations
â”‚   â”‚   â””â”€â”€ processors.py            # Data processing and validation
â”‚   â”œâ”€â”€ analysis/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ statistics.py            # Statistical analysis with pandas
â”‚   â”‚   â”œâ”€â”€ trends.py                # Trend analysis and calculations
â”‚   â”‚   â””â”€â”€ reports.py               # HTML report generation
â”‚   â”œâ”€â”€ cli/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ interface.py             # Main CLI interface
â”‚   â”‚   â”œâ”€â”€ commands/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ scrape_commands.py   # Scraping CLI commands
â”‚   â”‚   â”‚   â”œâ”€â”€ analysis_commands.py # Analysis CLI commands
â”‚   â”‚   â”‚   â””â”€â”€ db_commands.py       # Database CLI commands
â”‚   â”‚   â””â”€â”€ utils/
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ config.py            # Configuration management
â”‚   â”‚       â”œâ”€â”€ logger.py            # Logging utilities
â”‚   â”‚       â””â”€â”€ helpers.py           # Helper functions
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ settings.yaml                # Main configuration file
â”‚   â””â”€â”€ scrapers.yaml                # Scraper-specific configurations
â”œâ”€â”€ data/
â”‚   â””â”€â”€ price_monitor.db             # SQLite database
â”œâ”€â”€ data_output/
â”‚   â”œâ”€â”€ raw/                         # Raw scraped data
â”‚   â”œâ”€â”€ processed/                   # Processed and cleaned data
â”‚   â””â”€â”€ reports/                     # Generated HTML reports
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ user_guide.md                # User documentation
â”‚   â”œâ”€â”€ api_reference.md             # API documentation
â”‚   â””â”€â”€ ARCHITECTURE_DESIGN.md       # Technical architecture
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/                        # Unit tests
â”‚   â”œâ”€â”€ integration/                 # Integration tests
â”‚   â””â”€â”€ fixtures/                    # Test data fixtures
â”œâ”€â”€ logs/                            # Application logs
â”œâ”€â”€ main.py                          # Main application entry point
â”œâ”€â”€ test_implementation.py           # Comprehensive test suite
â”œâ”€â”€ bulk_data_generator.py           # Sample data generator
â”œâ”€â”€ scraping_final_video.mp4         # ğŸ“¹ Video demonstration
â”œâ”€â”€ requirements.txt                 # Python dependencies
â”œâ”€â”€ setup.py                         # Package setup
â””â”€â”€ README.md                        # This file
```

## Installation

1. **Clone the repository:**
   ```bash
   git clone https://github.com/Revaz-Goguadze/Scraping-f-project.git
   cd final-project
   ```

2. **Create virtual environment:**
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

4. **Setup database:**
   ```bash
   python main.py  # Initializes database on first run
   ```

## ğŸ¬ Quick Start Demo

**Want to see it in action first?** Watch the embedded video demonstration above, then try it yourself:

```bash
python main.py                    # Interactive scraping
python view_results.py            # View results and generate reports
```

## Usage Examples

### Basic Scraping

**Scrape all configured sites:**
```bash
python -m src.cli.interface scrape run
```

**Scrape specific site with custom workers:**
```bash
python -m src.cli.interface scrape run --site amazon --workers 4 --limit 10
```

**Scrape with rate limiting:**
```bash
python -m src.cli.interface scrape run --site ebay --delay 2.0 --workers 2
```

### Data Analysis

**Generate comprehensive HTML report:**
```bash
python -m src.cli.interface analyze generate-report --type comprehensive
```

**Analyze specific product:**
```bash
python -m src.cli.interface analyze product 1
```

**Find most volatile products:**
```bash
python -m src.cli.interface analyze volatility --top-n 10
```

**Analyze price trends:**
```bash
python -m src.cli.interface analyze trend 1 --days 30
```

### Sample Data Generation

**Generate test data:**
```bash
python bulk_data_generator.py
```

**Run comprehensive system test:**
```bash
python test_implementation.py
```

## Command Reference

### Scraping Commands

```bash
# Run all scrapers
python -m src.cli.interface scrape run

# Run specific site
python -m src.cli.interface scrape run --site [amazon|ebay|walmart]

# Configure workers and limits
python -m src.cli.interface scrape run --workers 4 --limit 50 --delay 1.5

# Run with specific configuration
python -m src.cli.interface scrape run --config-dir ./config
```

### Analysis Commands

```bash
# Generate reports
python -m src.cli.interface analyze generate-report --type comprehensive
python -m src.cli.interface analyze generate-report --type summary

# Product analysis
python -m src.cli.interface analyze product <product_id>
python -m src.cli.interface analyze trend <product_id> --days 30

# Market analysis
python -m src.cli.interface analyze volatility --top-n 10
```

### Configuration

**Main settings** (`config/settings.yaml`):
```yaml
database:
  url: "sqlite:///data/price_monitor.db"
  echo: false

scraping:
  default_delay: 1.0
  max_retries: 3
  timeout: 30
  concurrent_workers: 2

logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
```

**Scraper configuration** (`config/scrapers.yaml`):
```yaml
amazon:
  type: "static"
  selectors:
    title: "#productTitle"
    price: ".a-price-whole, .a-price .a-offscreen"
    availability: "#availability span"

ebay:
  type: "static"
  selectors:
    title: "h1#x-title-label-lbl"
    price: ".price-current, .price-now, .price-display"
    availability: "#availability-info, .availability"

walmart:
  type: "selenium"
  selectors:
    title: "h1[data-automation-id='product-title']"
    price: "[data-automation-id='price'] span"
    availability: "[data-automation-id='fulfillment-summary']"
```

## Output Examples

### Database Statistics
- **Products**: 222 across multiple categories
- **Sites**: 7 e-commerce platforms  
- **Price Records**: 36,246+ historical entries
- **Categories**: Electronics, Books, Clothing, Home, Sports, etc.

### Generated Reports
- **HTML Reports**: Professional reports with embedded charts (497KB)
- **Data Exports**: CSV, JSON, Excel formats
- **Visualizations**: Price trends, volatility analysis, site comparisons

### CLI Output Example
```bash
$ python -m src.cli.interface analyze volatility --top-n 5

--- Top 5 Most Volatile Products ---
     product_id                        product_name  mean_price  std_dev_price  volatility_coeff
196         217               HP Speaker Model 9865  496.27     519.69         1.047
128         149  Under Armour Basketball Model 9346  172.14     171.45         0.996
193         214        Under Armour Bike Model 3757  169.82     163.35         0.962
184         205    Mattel Remote Control Model 7413   56.67      53.53         0.945
44           65        Unilever Bandages Model 6902   31.73      29.80         0.939
```

## ğŸ“¹ Video Documentation

This project includes a comprehensive video demonstration (embedded above) showing all system capabilities in action. The video serves as both documentation and proof of functionality, meeting the Project.md requirement for video deliverables.

**ğŸ¯ Video demonstrates full compliance with project requirements:**
- âœ… Multi-source scraping (Amazon, eBay, Shop.ge)
- âœ… Both static and dynamic scraping methods
- âœ… Concurrent processing capabilities
- âœ… Database storage and management
- âœ… Statistical analysis and reporting
- âœ… Professional CLI interface
- âœ… HTML report generation with visualizations

## Bonus Features Implemented

This project successfully implements **6 out of 7 bonus features**, earning the **maximum 5 bonus points** available.

- **âœ… Advanced Anti-Bot Handling (2 points)**: User-agent rotation, proxy support, retry logic
- **âœ… Real-time Monitoring (1 point)**: Scheduled scraping and price change alerts
- **âœ… Advanced Data Analysis (1 point)**: Statistical analysis, volatility, and trend detection
- **âœ… Mobile-Respoclearnsive Reports (1 point)**: HTML reports with responsive CSS
- **âœ… Performance Optimization (1 point)**: Concurrent processing, caching, memory management
- **âœ… Docker Implementation (1 point)**: Full containerization with Docker and Docker Compose

## ğŸš€ Getting Started

1. **ğŸ“¹ Watch the video**: Review the embedded demonstration above for a complete walkthrough
2. **âš™ï¸ Install dependencies**: Follow the installation steps above
3. **ğŸ•·ï¸ Start scraping**: Run `python main.py` and select a site
4. **ğŸ“Š View results**: Run `python view_results.py` to see your data
5. **ğŸ“‹ Generate reports**: Use CLI commands for advanced analysis

---

